{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df = pd.read_csv('/Users/anmol_gorakshakar/python/machine_learning/heart_disease/heart_disease.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f856eaa10a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.HeartDisease.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleanup(data):\n",
    "    working_data = data.copy()\n",
    "    index = {}\n",
    "    for header in working_data:\n",
    "        if working_data[header].dtype == 'O':\n",
    "            unique_values_in_column = list(working_data[header].unique())\n",
    "            column_indexer = {k:v for k,v in enumerate(unique_values_in_column)}\n",
    "            index[header] = column_indexer\n",
    "            working_data[header] = working_data[header].replace({v:k for k,v in column_indexer.items()})\n",
    "    \n",
    "    return working_data, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data , index = df_cleanup(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "numerical_cols = [i for i in working_data.columns if i not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = working_data.pop('HeartDisease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import scipy.stats as ss\n",
    "\n",
    "def conditional_entropy(x,y):\n",
    "    y_counter = Counter(y)\n",
    "    xy_counter = Counter(list(zip(x,y)))\n",
    "    total_occurances = sum(y_counter.values())\n",
    "    entropy = 0.\n",
    "    for xy in xy_counter.keys():\n",
    "        p_y = y_counter[xy[1]]/total_occurances\n",
    "        p_xy = xy_counter[xy]/total_occurances\n",
    "        entropy += p_xy * math.log(p_y/ p_xy, math.e)\n",
    "    return entropy\n",
    "\n",
    "def theils_u(x,y):\n",
    "    # coeff = (H(X) - H(X|Y)) / H(X) \n",
    "    h_xy = conditional_entropy(x,y)\n",
    "    x_counter = Counter(x)\n",
    "    total_occurances = sum(x_counter.values())\n",
    "    p_x = list(map(lambda x: x / total_occurances, x_counter.values()))\n",
    "    h_x = ss.entropy(p_x)\n",
    "    if h_x == 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return (h_x - h_xy)/h_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_for_dataframe(df, label, header = None):\n",
    "    if header == None:\n",
    "        header = df.columns\n",
    "    d = {}\n",
    "    for h in header:\n",
    "        d[h] = theils_u(df[h], label)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_corr(x,y):\n",
    "    r = ss.pointbiserialr(x, y).correlation\n",
    "    return r\n",
    "\n",
    "def calculate_entropy_for_dataframe_numerical_data(df, label, header = None):\n",
    "    if header == None:\n",
    "        header = df.columns\n",
    "    d = {}\n",
    "    for h in header:\n",
    "        d[h] = numerical_corr(df[h], label)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_data = {}\n",
    "correlation_data.update(calculate_entropy_for_dataframe(working_data, \n",
    "                                                        label, \n",
    "                                                        header=categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_data.update(calculate_entropy_for_dataframe_numerical_data(working_data, \n",
    "                                                                       label, \n",
    "                                                                       header=numerical_cols[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sifnificant_cols = [k for k,v in correlation_data.items() if abs(v)>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChestPainType',\n",
       " 'ExerciseAngina',\n",
       " 'ST_Slope',\n",
       " 'Age',\n",
       " 'RestingBP',\n",
       " 'Cholesterol',\n",
       " 'FastingBS',\n",
       " 'MaxHR',\n",
       " 'Oldpeak']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sifnificant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def preprocess_categorical(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    cols = df.columns\n",
    "    for col in cols:\n",
    "        categorical_dict = {}\n",
    "        if df[col].dtype == 'object':\n",
    "            unique_values = {v:k for k,v in zip(range(len(df[col].unique())), df[col].unique())}\n",
    "            categorical_dict[col] = unique_values\n",
    "            print(unique_values)\n",
    "            df[col] = df[col].apply(lambda x: unique_values[x])\n",
    "    return df\n",
    "\n",
    "processed_df = preprocess_categorical(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test = train_test_split(processed_df, \n",
    "                         test_size = 0.1,\n",
    "                         stratify=processed_df.HeartDisease)\n",
    "\n",
    "X_train_label = X_train.HeartDisease\n",
    "X_train = X_train[[i for i in X_train if i!='HeartDisease']]\n",
    "\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],1,X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LSTM, Flatten, MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=64, input_shape=(1, 11), return_sequences=True))\n",
    "# model.add(LSTM(units=64, input_shape=(1, 5), return_sequences=True))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(LSTM(units=256, return_sequences=True))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(LSTM(units=512, return_sequences=True))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=1024, kernel_size=(1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(1)))\n",
    "\n",
    "model.add(Conv1D(filters=2048, kernel_size=(1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=(1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"Adamax\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "cb = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "hst = model.fit(tf.cast(X_train, tf.float32), \n",
    "                tf.cast(X_train_label, tf.int8),\n",
    "                validation_split=0.1, \n",
    "                epochs =25,\n",
    "                batch_size=16, \n",
    "                shuffle=True,\n",
    "                callbacks=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test_label = X_test.HeartDisease\n",
    "X_test = X_test[[i for i in X_test if i!='HeartDisease']]\n",
    "X_test = X_test.to_numpy()\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],1,X_test.shape[1]))\n",
    "model.evaluate(tf.cast(X_test, tf.float32), \n",
    "                tf.cast(X_test_label, tf.int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted = []\n",
    "for i in range(df.shape[0]):\n",
    "    p = model.predict(np.reshape(np.array(processed_df.loc[i][:-1]), (1,1,11)))[0][0][0]\n",
    "    predicted.append(p)\n",
    "    \n",
    "df['predicted'] = [*map(round ,predicted)]\n",
    "series = df.apply(lambda x: x['HeartDisease']==x['predicted'] , axis=1)\n",
    "series.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# round3\n",
    "epochs = len(hst.epoch)\n",
    "accuracy = hst.history['accuracy']\n",
    "loss = hst.history['loss']\n",
    "val_loss = hst.history['val_loss']\n",
    "val_accuracy = hst.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(17, 7))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(epochs), accuracy, label='Training Accuracy')\n",
    "plt.plot(range(epochs), val_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy : Training vs. Validation ')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(epochs), loss, label='Training Loss')\n",
    "plt.plot(range(epochs), val_loss, label='Validation Loss')\n",
    "plt.title('Loss : Training vs. Validation ')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #for plotting\n",
    "from sklearn.ensemble import RandomForestClassifier #for the model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz #plot tree\n",
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from sklearn.metrics import classification_report #for model evaluation\n",
    "from sklearn.metrics import confusion_matrix #for model evaluation\n",
    "from sklearn.model_selection import train_test_split #for data splitting\n",
    "import eli5 #for purmutation importance\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import shap #for SHAP values\n",
    "from pdpbox import pdp, info_plots #for partial plots\n",
    "np.random.seed(123) #ensure reproducibility\n",
    "\n",
    "pd.options.mode.chained_assignment = None  #hide any pandas warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_from_dataframe(df, label, batch_size=32):\n",
    "    data = df.copy()\n",
    "    data = tf.data.Dataset.from_tensor_slices((dict(df), label))\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(batch_size)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_encoding_layer(feature, dataset, dtype, max_tokens):\n",
    "    index = pp.IntegerLookup(max_values=max_tokens, oov_value=-2) # integer lookup initialization \n",
    "    feature_ds = dataset.map(lambda x, y: x[feature]) # slice dataset to get the feature column\n",
    "    index.adapt(feature_ds) # analyze the feature column to generate a vocabulary \n",
    "    encoder = pp.CategoryEncoding(max_tokens=len(index.get_vocabulary())) # generate an encoding layer \n",
    "    return lambda feature: encoder(index(feature)) #returns a encoder which takes feature as argument\n",
    "\n",
    "def get_normalization_layer(feature, dataset):\n",
    "    normalizer = pp.Normalization(axis=None)\n",
    "    feature_ds = dataset.map(lambda x, y: x[feature])\n",
    "    normalizer.adapt(feature_ds)\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = working_data[sifnificant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2e08565b6178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train, test, train_label, test_label = train_test_split(ds, label, test_size=0.2)\n",
    "train, val, train_label, val_label = train_test_split(train, train_label, test_size=0.2)\n",
    "\n",
    "train_ds = dataset_from_dataframe(train, train_label)\n",
    "val_ds = dataset_from_dataframe(val, val_label)\n",
    "test_ds = dataset_from_dataframe(test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import preprocessing as pp\n",
    "all_inputs = []\n",
    "encoded_inputs = []\n",
    "\n",
    "for header in categorical_cols:\n",
    "    print('processing', header)\n",
    "    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int32')\n",
    "    encoding_layer = get_categorical_encoding_layer(feature=header, \n",
    "                                              dataset=train_ds,\n",
    "                                              dtype='int32',\n",
    "                                              max_tokens=5 #random_number do optimize\n",
    "                                             )\n",
    "    print('got encoding_layer')\n",
    "    encoded_categorical_col = encoding_layer(categorical_col)\n",
    "    all_inputs.append(categorical_col)\n",
    "    encoded_inputs.append(encoded_categorical_col)\n",
    "    \n",
    "for header in numerical_cols:\n",
    "    numerical_col = tf.keras.Input(shape=(1,), name=header)\n",
    "    normalization_layer = get_normalization_layer(feature=header, \n",
    "                                                  dataset=train_ds\n",
    "                                                 )\n",
    "    normalized_numerical_layer = normalization_layer(numerical_col)\n",
    "    all_inputs.append(numerical_col)\n",
    "    encoded_inputs.append(normalized_numerical_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.apply(lambda x, y: print(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
